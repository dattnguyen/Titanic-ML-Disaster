{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File C:\\Users\\Dat Nguyen\\Kaggle\\data\\train.csv does not exist: 'C:\\\\Users\\\\Dat Nguyen\\\\Kaggle\\\\data\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-172e10d76c0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# read .csv data file to pandas data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File C:\\Users\\Dat Nguyen\\Kaggle\\data\\train.csv does not exist: 'C:\\\\Users\\\\Dat Nguyen\\\\Kaggle\\\\data\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier,Pool\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# get the absolute path of the running python file\n",
    "code_filepath = os.path.abspath(\"__file__\")\n",
    "\n",
    "titanic_dir = os.path.dirname(os.path.dirname(code_filepath))\n",
    "\n",
    "# get the data file\n",
    "data_filepath = os.path.join(titanic_dir, \"data\", \"train.csv\")\n",
    "\n",
    "# read .csv data file to pandas data frame\n",
    "train_df = pd.read_csv(data_filepath)\n",
    "\n",
    "\n",
    "test_path= os.path.join(titanic_dir, \"data\", \"test.csv\")\n",
    "test_df= pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null \n",
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na values in Cabin with Unknown:\n",
    "for d in [train_df, test_df]:\n",
    "    d['Cabin'].fillna('Unknown', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing age with passengers average age\n",
    "data= pd.concat([train_df,test_df], sort=False)\n",
    "for d in [train_df, test_df]:\n",
    "    d['Age'].fillna(data['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill na values in Embarked columns:\n",
    "for d in [train_df, test_df]:\n",
    "    d['Embarked'].fillna(data['Embarked'].mode()[0], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create family size features by combining SibSp and Parch:\n",
    "for d in [train_df, test_df]:\n",
    "    d['Family_size']= d['SibSp']+ d['Parch']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting Initial from Names: \n",
    "for d in [train_df, test_df]:\n",
    "    d['Title'] = d['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "    d['Title'] = d['Title'].replace('Ms', 'Miss')\n",
    "    d['Title'] = d['Title'].replace('Mlle', 'Miss')\n",
    "    d['Title'] = d['Title'].replace('Mme', 'Mrs')\n",
    "    d['Title'] = d['Title'].apply(lambda x: 'Other' if ((x != 'Mr') and (x != 'Miss') and (x != 'Mrs')) else x)#\n",
    "print(train_df['Title'].value_counts().sort_values())\n",
    "print(test_df['Title'].value_counts().sort_values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Decks from Cabin columns\n",
    "for d in [train_df, test_df]:\n",
    "    d['Decks']= d['Cabin'].str.extract('([A-Z])', expand=False)\n",
    "    print(d['Decks'].value_counts())\n",
    "train_df[train_df['Decks']=='T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [train_df, test_df]:\n",
    "\n",
    "    for index, value in d['Decks'].iteritems():\n",
    "        if value in ['A','B','C']:\n",
    "            d.loc[index,'Deck_level']= 'Top'\n",
    "        elif value in ['D','E']:\n",
    "            d.loc[index,'Deck_level']= 'Upper'\n",
    "        elif value == 'F':\n",
    "            d.loc[index,'Deck_level']= 'Middle'\n",
    "        elif value == 'U':\n",
    "            d.loc[index,'Deck_level']= 'Unknown'\n",
    "        else: \n",
    "            d.loc[index,'Deck_level']= 'Low'\n",
    "print(train_df['Deck_level'].value_counts())\n",
    "print(test_df['Deck_level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [train_df, test_df]:\n",
    "\n",
    "    for index, value in d['Age'].iteritems():\n",
    "        if value<=15:\n",
    "            d.loc[index,'Age_group']= 'Children'\n",
    "        elif 15< value <= 50:\n",
    "            d.loc[index,'Age_group']= 'Adults'\n",
    "        else: \n",
    "            d.loc[index,'Age_group']= 'Elder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Fare'].fillna(test_df['Fare'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking 5 models:\n",
    "## 2 Trees: Catboost, LightGBM\n",
    "## 2 Non-trees: KNN, Logistic Regression\n",
    "## 1 NNs: MLPClassifier\n",
    "## Metamodel: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost & Light GBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_train= train_df.drop(['Survived','Ticket','PassengerId','Name','SibSp','Parch','Decks'], axis=1)\n",
    "tree_label= train_df['Survived']\n",
    "tree_test= test_df.drop(['Ticket','PassengerId','Name','SibSp','Parch','Decks'], axis=1)\n",
    "catdims= ['Pclass','Sex', 'Cabin', 'Embarked', 'Title', 'Deck_level', 'Age_group']\n",
    "\n",
    "for d in [tree_train,tree_test]:\n",
    "    for col in catdims:\n",
    "        d[col]=d[col].astype('category')\n",
    "    \n",
    "\n",
    "print(tree_train.head())\n",
    "print(tree_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN, Logitistic Regresssions & NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try again with less features:\n",
    "ntree_train = train_df.drop(['Survived','Name','PassengerId','SibSp','Parch','Cabin','Ticket','Decks','Title'], axis= 1)\n",
    "ntree_test = test_df.drop(['Name','PassengerId','SibSp','Parch','Cabin','Ticket','Decks','Title'], axis= 1)\n",
    "ntree_label= train_df['Survived']\n",
    "ntree_train= pd.get_dummies(ntree_train)\n",
    "ntree_test= pd.get_dummies(ntree_test)\n",
    "\n",
    "scaler= preprocessing.StandardScaler()\n",
    "\n",
    "train_scaled= scaler.fit_transform(ntree_train)\n",
    "\n",
    "test_scaled= scaler.fit_transform(ntree_test)\n",
    "ntree_train_scaled= pd.DataFrame(train_scaled, columns= ntree_train.columns)\n",
    "ntree_test_scaled= pd.DataFrame(test_scaled, columns=ntree_test.columns)\n",
    "\n",
    "ntree_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_stacking(X, Y, X_test, model, cv = 4, name_model_as_feature = 'logistic', \n",
    "                      catdims= None, catboost=False, lightgbm=False, random_state=False):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle = True)\n",
    "    accuracy_list=[]\n",
    "    # Create an empty dictionary to keep k-fold\n",
    "    train_splits = {}\n",
    "    # Loop for k-fold index to generate X_train, X_val, Y_train, Y_val of each fold\n",
    "    for (i, (train_index, val_index)) in enumerate(skf.split(X, Y)):\n",
    "        train_splits['Fold_' + str(i + 1)] = (train_index, val_index, X.iloc[train_index], X.iloc[val_index], \n",
    "                                              Y.iloc[train_index], Y.iloc[val_index])\n",
    "    # Create an empty DataFrame to keep features stacking of training data\n",
    "    stacking_train = pd.DataFrame(columns = [name_model_as_feature])\n",
    "    # Create an empty array to compute sum of probability of test data\n",
    "    sum_proba_test = np.zeros(X_test.shape[0])\n",
    "    # Loop for k_fold\n",
    "    for i in range(cv):\n",
    "        # Retrieve infomation of fold_i+1\n",
    "        (train_index, val_index, X_train, X_val, Y_train, Y_val) = train_splits['Fold_' + str(i + 1)]\n",
    "        # Set random_state for the model and fit\n",
    "        if random_state:    \n",
    "            model.set_params(random_state = np.random.randint(2019))\n",
    "        if catboost:\n",
    "            model.set_params(cat_features=catdims)\n",
    "        if lightgbm:\n",
    "            model.set_params(categorical_feature=catdims)\n",
    "        model.fit(X_train, Y_train)\n",
    "        # Append results on X_val to stacking_train DataFrame\n",
    "        stacking_train = pd.concat([stacking_train, pd.DataFrame(model.predict_proba(X_val)[:,1], columns = [name_model_as_feature], index = val_index)], axis = 0)\n",
    "        # Compute predict_proba of X_test\n",
    "        sum_proba_test += model.predict_proba(X_test)[:,1]\n",
    "        y_pred= model.predict(X_val)\n",
    "        accuracy = metrics.accuracy_score(Y_val, y_pred)\n",
    "        accuracy_list.append(accuracy)\n",
    "        accuracy= np.mean(accuracy_list)\n",
    "    print('Model accuracy:', accuracy)    \n",
    "    \n",
    "    # Reset index of stacking_train\n",
    "    stacking_train = stacking_train.sort_index()\n",
    "        \n",
    "    # Create a DataFrame to keep stacking_test\n",
    "    stacking_test = pd.DataFrame(data = sum_proba_test/cv, columns = [name_model_as_feature])\n",
    "    return (stacking_train, stacking_test)\n",
    "  \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model = LogisticRegression(C=100)\n",
    "logistic = features_stacking(ntree_train_scaled, ntree_label, ntree_test_scaled, model, cv = 4, name_model_as_feature = 'logistic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "model = KNeighborsClassifier()\n",
    "knn = features_stacking(ntree_train_scaled, ntree_label, ntree_test_scaled,model, cv = 4, name_model_as_feature = 'knn', random_state = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "model = SVC(probability = True, gamma='scale')\n",
    "svm = features_stacking(ntree_train_scaled, ntree_label, ntree_test_scaled, model, cv = 4, name_model_as_feature = 'svm', random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "nn = features_stacking(ntree_train_scaled, ntree_label, ntree_test_scaled, model, cv = 4, name_model_as_feature = 'nn', random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_catdims=[0,1,4,5,7,8,9]\n",
    "model = CatBoostClassifier()\n",
    "cb = features_stacking(tree_train, tree_label, tree_test, \n",
    "                       model, cv = 4, name_model_as_feature = 'cb', random_state = False,catboost=True, catdims=cb_catdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM\n",
    "model = LGBMClassifier(learning_rate=0.01, n_estimators=1000, num_leaves=10)\n",
    "gbm = features_stacking(tree_train, tree_label, tree_test, \n",
    "                       model, cv = 4, name_model_as_feature = 'gbm', random_state = 2019,lightgbm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_train = pd.concat([knn[0], svm[0], nn[0], cb[0], gbm[0]], axis = 1)\n",
    "stacking_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_test = pd.concat( [knn[1], svm[1], nn[1], cb[1], gbm[1]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(stacking_train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle= True,random_state=1906)\n",
    "params = {'penalty': ['l1','l2'], \n",
    "                  'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "gs_clf = GridSearchCV(clf , param_grid = params, cv=k_fold, scoring=\"accuracy\", verbose = 1, n_jobs=-1)\n",
    "gs_clf.fit(stacking_train, train_df['Survived'])\n",
    "\n",
    "clf_best = gs_clf.best_estimator_\n",
    "# Best score\n",
    "gs_clf.best_score_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = clf_best.predict(stacking_test)\n",
    "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': Y_predict.astype(int)})\n",
    "submission.to_csv('stacking.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
